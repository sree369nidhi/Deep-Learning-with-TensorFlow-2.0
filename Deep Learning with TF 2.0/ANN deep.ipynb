{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ANN deep.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"A1ztRmAbu-rT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"7a309be0-4c73-4db8-cd5a-599eaba7a817","executionInfo":{"status":"ok","timestamp":1575210846641,"user_tz":-330,"elapsed":32918,"user":{"displayName":"Iyon Kingo","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDhq98QqJiXINH7ESjFH7z3vH9Es7Mv5M40Rn71=s64","userId":"04777749448336901890"}}},"source":["!pip install --upgrade tensorflow"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting tensorflow\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/0f/7bd55361168bb32796b360ad15a25de6966c9c1beb58a8e30c01c8279862/tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n","\u001b[K     |████████████████████████████████| 86.3MB 135kB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.1)\n","Collecting tensorboard<2.1.0,>=2.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 45.0MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n","Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n","Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.11.2)\n","Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n","Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.8)\n","Collecting tensorflow-estimator<2.1.0,>=2.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n","\u001b[K     |████████████████████████████████| 450kB 79.8MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n","Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.1.8)\n","Requirement already satisfied, skipping upgrade: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n","Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.33.6)\n","Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n","Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.0)\n","Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.17.4)\n","Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (0.16.0)\n","Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (0.4.1)\n","Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (41.6.0)\n","Collecting google-auth<2,>=1.6.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/cb/786dc53d93494784935a62947643b48250b84a882474e714f9af5e1a1928/google_auth-1.7.1-py2.py3-none-any.whl (74kB)\n","\u001b[K     |████████████████████████████████| 81kB 12.9MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (2.21.0)\n","Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.1)\n","Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow) (2.8.0)\n","Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (1.3.0)\n","Requirement already satisfied, skipping upgrade: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.1)\n","Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (4.0)\n","Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (0.2.7)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (2019.9.11)\n","Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.0.4)\n","Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (2.8)\n","Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (1.24.3)\n","Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.0)\n","Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (0.4.7)\n","\u001b[31mERROR: tensorboard 2.0.2 has requirement grpcio>=1.24.3, but you'll have grpcio 1.15.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you'll have google-auth 1.7.1 which is incompatible.\u001b[0m\n","Installing collected packages: google-auth, tensorboard, tensorflow-estimator, tensorflow\n","  Found existing installation: google-auth 1.4.2\n","    Uninstalling google-auth-1.4.2:\n","      Successfully uninstalled google-auth-1.4.2\n","  Found existing installation: tensorboard 1.15.0\n","    Uninstalling tensorboard-1.15.0:\n","      Successfully uninstalled tensorboard-1.15.0\n","  Found existing installation: tensorflow-estimator 1.15.1\n","    Uninstalling tensorflow-estimator-1.15.1:\n","      Successfully uninstalled tensorflow-estimator-1.15.1\n","  Found existing installation: tensorflow 1.15.0\n","    Uninstalling tensorflow-1.15.0:\n","      Successfully uninstalled tensorflow-1.15.0\n","Successfully installed google-auth-1.7.1 tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["google"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"tabGfIOlu1_d","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"f1ed66ac-0289-4e26-bd01-67c43b8421d5","executionInfo":{"status":"ok","timestamp":1575210907319,"user_tz":-330,"elapsed":2399,"user":{"displayName":"Iyon Kingo","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDhq98QqJiXINH7ESjFH7z3vH9Es7Mv5M40Rn71=s64","userId":"04777749448336901890"}}},"source":["import tensorflow as tf\n","import numpy as np\n","tf.__version__"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.0.0'"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"6KNvlH-Nv2H2","colab_type":"code","colab":{}},"source":["num_classes = 10 # total classes (0-9 digits).\n","num_features = 784 # data features (img shape: 28*28).\n","\n","# Training parameters.\n","learning_rate = 0.001\n","training_steps = 3000\n","batch_size = 256\n","display_step = 100\n","\n","# Network parameters.\n","n_hidden_1 = 128 # 1st layer number of neurons.\n","n_hidden_2 = 256 # 2nd layer number of neurons."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DCMOL_9lv5nN","colab_type":"code","colab":{}},"source":["from tensorflow.keras.datasets import mnist\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n","x_train , x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])\n","x_train, x_test = x_train/255. , x_test/255.\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GzJUFyCQwdzC","colab_type":"code","colab":{}},"source":["train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","train_data = train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Kq5lUoPwtJM","colab_type":"code","colab":{}},"source":["rn = tf.initializers.RandomNormal()\n","\n","weights = {\n","    'h1' : tf.Variable(rn([num_features, n_hidden_1])),\n","    'h2' : tf.Variable(rn([n_hidden_1, n_hidden_2])),\n","    'out': tf.Variable(rn([n_hidden_2, num_classes]))\n","}\n","\n","bias = {\n","    'b1' : tf.Variable(tf.zeros([n_hidden_1])),\n","    'b2' : tf.Variable(tf.zeros([n_hidden_2])),\n","    'out': tf.Variable(tf.zeros([num_classes]))\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eSfFiKaKxjUV","colab_type":"code","colab":{}},"source":["def nn(x):\n","    l1 = tf.add(tf.matmul(x, weights['h1']), bias['b1'])\n","    l1 = tf.nn.sigmoid(l1)\n","\n","    l2 = tf.add(tf.matmul(l1, weights['h2']), bias['b2'])\n","    l2 = tf.nn.sigmoid(l2)\n","\n","    out = tf.add(tf.matmul(l2, weights['out']), bias['out'])\n","    return tf.nn.softmax(out)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1syH10-4yR-s","colab_type":"code","colab":{}},"source":["def cross_en(y_pred, y_true):\n","    y_true = tf.one_hot(y_true, depth=num_classes)\n","    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\n","    return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred)))\n","\n","def acc(y_pred, y_true):\n","    corr_pred = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n","    return tf.reduce_mean(tf.cast(corr_pred, tf.float32), axis=-1)\n","\n","optimizer = tf.optimizers.SGD(learning_rate)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MJpuuZi_01ig","colab_type":"code","colab":{}},"source":["def run_opt(x,y):\n","    with tf.GradientTape() as g:\n","        pred = nn(x)\n","        loss = cross_en(pred, y)\n","    trainable_vars = [weights.values() , bias.values()]\n","    gradients = g.gradient(loss,trainable_vars)\n","    optimizer.apply_gradients(zip(gradients, trainable_vars))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wcVWmhoT1cmu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":394},"outputId":"afef4c84-6aac-433b-fd5d-2a3516c720d4","executionInfo":{"status":"error","timestamp":1575210788065,"user_tz":-330,"elapsed":1484,"user":{"displayName":"Iyon Kingo","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDhq98QqJiXINH7ESjFH7z3vH9Es7Mv5M40Rn71=s64","userId":"04777749448336901890"}}},"source":["for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n","    run_opt(batch_x, batch_y)\n","\n","    if step % display_step == 0:\n","        pred = nn(batch_x)\n","        loss = cross_en(pred, batch_y)\n","        accu = acc(pred, batch_y)\n","        print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, accu))\n","        "],"execution_count":28,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-6cf791e9d42d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mrun_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdisplay_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-27-7120654b90fd>\u001b[0m in \u001b[0;36mrun_opt\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrainable_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainable_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_slots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m       \u001b[0mapply_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m       return distribute_ctx.get_replica_context().merge_call(\n\u001b[1;32m    439\u001b[0m           \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distributed_apply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapply_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_prepare\u001b[0;34m(self, var_list)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m       var_devices = (getattr(var, \"devices\", None) or  # Distributed\n\u001b[0;32m--> 605\u001b[0;31m                      [var.device])                     # Regular\n\u001b[0m\u001b[1;32m    606\u001b[0m       \u001b[0mvar_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mvar_device\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_devices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'dict_values' object has no attribute 'device'"]}]},{"cell_type":"code","metadata":{"id":"4gUPQ-Yy3sQq","colab_type":"code","colab":{}},"source":["\n","# A random value generator to initialize weights.\n","random_normal = tf.initializers.RandomNormal()\n","\n","weights = {\n","    'h1': tf.Variable(random_normal([num_features, n_hidden_1])),\n","    'h2': tf.Variable(random_normal([n_hidden_1, n_hidden_2])),\n","    'out': tf.Variable(random_normal([n_hidden_2, num_classes]))\n","}\n","biases = {\n","    'b1': tf.Variable(tf.zeros([n_hidden_1])),\n","    'b2': tf.Variable(tf.zeros([n_hidden_2])),\n","    'out': tf.Variable(tf.zeros([num_classes]))\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mhFt3vQF31vA","colab_type":"code","colab":{}},"source":["def nn(x):\n","    l1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n","    l1 = tf.nn.sigmoid(l1)\n","\n","    l2 = tf.add(tf.matmul(l1, weights['h2']), biases['b2'])\n","    l2 = tf.nn.sigmoid(l2)\n","\n","    out = tf.add(tf.matmul(l2, weights['out']), biases['out'])\n","    return tf.nn.softmax(out)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zmDo9WxT39On","colab_type":"code","colab":{}},"source":["def cross_en(y_pred, y_true):\n","    y_true = tf.one_hot(y_true, depth=num_classes)\n","    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\n","    return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred)))\n","\n","def acc(y_pred, y_true):\n","    corr_pred = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n","    return tf.reduce_mean(tf.cast(corr_pred, tf.float32), axis=-1)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OAGoENFv2AjB","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yU-q2tsS3Ues","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":251},"outputId":"8f75e104-cd37-46a8-bdee-fb5b2b9fd389","executionInfo":{"status":"error","timestamp":1575215885067,"user_tz":-330,"elapsed":888,"user":{"displayName":"Iyon Kingo","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDhq98QqJiXINH7ESjFH7z3vH9Es7Mv5M40Rn71=s64","userId":"04777749448336901890"}}},"source":["for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n","    # Run the optimization to update W and b values.\n","    optimizer = tf.optimizers.SGD(learning_rate).minimize(loss)\n","    \n","    if step % display_step == 0:\n","        pred = nn(batch_x)\n","        loss = cross_en(pred, batch_y)\n","        acc = acc(pred, batch_y)\n","        print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))"],"execution_count":17,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-59ba9c08894f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Run the optimization to update W and b values.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdisplay_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'loss' is not defined"]}]},{"cell_type":"code","metadata":{"id":"HQxr3u0e3Ubg","colab_type":"code","colab":{}},"source":["# MNIST dataset parameters.\n","num_classes = 10 # total classes (0-9 digits).\n","num_features = 784 # data features (img shape: 28*28).\n","\n","# Training parameters.\n","learning_rate = 0.001\n","training_steps = 3000\n","batch_size = 256\n","display_step = 100\n","\n","# Network parameters.\n","n_hidden_1 = 128 # 1st layer number of neurons.\n","n_hidden_2 = 256 # 2nd layer number of neurons."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o2X6La0uL5om","colab_type":"code","colab":{}},"source":["from tensorflow.keras.datasets import mnist\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","# Convert to float32.\n","x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n","# Flatten images to 1-D vector of 784 features (28*28).\n","x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])\n","# Normalize images value from [0, 255] to [0, 1].\n","x_train, x_test = x_train / 255., x_test / 255."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nMhB6AewL7RM","colab_type":"code","colab":{}},"source":["train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","train_data = train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SdxqpcbJL9UK","colab_type":"code","colab":{}},"source":["# A random value generator to initialize weights.\n","random_normal = tf.initializers.RandomNormal()\n","\n","weights = {\n","    'h1': tf.Variable(random_normal([num_features, n_hidden_1])),\n","    'h2': tf.Variable(random_normal([n_hidden_1, n_hidden_2])),\n","    'out': tf.Variable(random_normal([n_hidden_2, num_classes]))\n","}\n","biases = {\n","    'b1': tf.Variable(tf.zeros([n_hidden_1])),\n","    'b2': tf.Variable(tf.zeros([n_hidden_2])),\n","    'out': tf.Variable(tf.zeros([num_classes]))\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"z11PiS42L_oK","colab_type":"code","colab":{}},"source":["# Create model.\n","def neural_net(x):\n","    # Hidden fully connected layer with 128 neurons.\n","    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n","    # Apply sigmoid to layer_1 output for non-linearity.\n","    layer_1 = tf.nn.sigmoid(layer_1)\n","    \n","    # Hidden fully connected layer with 256 neurons.\n","    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n","    # Apply sigmoid to layer_2 output for non-linearity.\n","    layer_2 = tf.nn.sigmoid(layer_2)\n","    \n","    # Output fully connected layer with a neuron for each class.\n","    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n","    # Apply softmax to normalize the logits to a probability distribution.\n","    return tf.nn.softmax(out_layer)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1KVraBSLMBix","colab_type":"code","colab":{}},"source":["# Cross-Entropy loss function.\n","def cross_entropy(y_pred, y_true):\n","    # Encode label to a one hot vector.\n","    y_true = tf.one_hot(y_true, depth=num_classes)\n","    # Clip prediction values to avoid log(0) error.\n","    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\n","    # Compute cross-entropy.\n","    return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred)))\n","\n","# Accuracy metric.\n","def accuracy(y_pred, y_true):\n","    # Predicted class is the index of highest score in prediction vector (i.e. argmax).\n","    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n","    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis=-1)\n","\n","# Stochastic gradient descent optimizer.\n","optimizer = tf.optimizers.SGD(learning_rate)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"thHGSh_0MDuH","colab_type":"code","colab":{}},"source":["def run_optimization(x, y):\n","    # Wrap computation inside a GradientTape for automatic differentiation.\n","    with tf.GradientTape() as g:\n","        pred = neural_net(x)\n","        loss = cross_entropy(pred, y)\n","        \n","    # Variables to update, i.e. trainable variables.\n","    trainable_variables = weights.values() + biases.values()\n","\n","    # Compute gradients.\n","    gradients = g.gradient(loss, trainable_variables)\n","    \n","    # Update W and b following gradients.\n","    optimizer.apply_gradients(zip(gradients, trainable_variables))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dEh9fES_MF0U","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":370},"outputId":"6ec347aa-89be-425e-e6d2-a9b6e94dc013","executionInfo":{"status":"error","timestamp":1575215990910,"user_tz":-330,"elapsed":1307,"user":{"displayName":"Iyon Kingo","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDhq98QqJiXINH7ESjFH7z3vH9Es7Mv5M40Rn71=s64","userId":"04777749448336901890"}}},"source":["for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n","    # Run the optimization to update W and b values.\n","    run_optimization(batch_x, batch_y)\n","    \n","    if step % display_step == 0:\n","        pred = neural_net(batch_x)\n","        loss = cross_entropy(pred, batch_y)\n","        acc = accuracy(pred, batch_y)\n","        print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))"],"execution_count":25,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-6562c31e3a43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Run the optimization to update W and b values.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mrun_optimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdisplay_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-24-fd3bfc60346e>\u001b[0m in \u001b[0;36mrun_optimization\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Variables to update, i.e. trainable variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrainable_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Compute gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'dict_values' and 'dict_values'"]}]},{"cell_type":"code","metadata":{"id":"bIe__U0yMIKx","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}